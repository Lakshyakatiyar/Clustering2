{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa3187e-af2b-449a-a6ea-dfefd3717730",
   "metadata": {},
   "source": [
    "1.Hierarchical clustering:\n",
    "Hierarchical clustering is a method of cluster analysis that builds a hierarchy of clusters. Unlike partitioning methods like K-means, hierarchical clustering does not require specifying the number of clusters beforehand. It creates a dendrogram, which illustrates the arrangement of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d401e24-7d25-4e67-94f5-171c5e6789ab",
   "metadata": {},
   "source": [
    "2.Two main types of hierarchical clustering algorithms:\n",
    "\n",
    "Agglomerative hierarchical clustering: Starts with each point as a single cluster and iteratively merges the closest pairs of clusters until only one cluster remains.\n",
    "Divisive hierarchical clustering: Starts with all data points in one cluster and recursively splits the cluster into smaller clusters until each data point is in its own cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075da1b-aa58-4440-96dd-a69754345202",
   "metadata": {},
   "source": [
    "3.Determining distance between clusters:\n",
    "The distance between two clusters is determined based on a chosen distance metric, such as:\n",
    "\n",
    "Euclidean distance\n",
    "Manhattan distance\n",
    "Cosine similarity\n",
    "Pearson correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bf6aa-0a5b-44a0-8ee2-ec2b5029ba80",
   "metadata": {},
   "source": [
    "4.Determining the optimal number of clusters:\n",
    "Common methods for determining the optimal number of clusters in hierarchical clustering include:\n",
    "\n",
    "Observation of dendrogram structure\n",
    "Height of the dendrogram\n",
    "Elbow method\n",
    "Gap statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2b054-bbe2-4036-bb3c-9d27c632c3c6",
   "metadata": {},
   "source": [
    "5.Dendrograms:\n",
    "Dendrograms are tree-like diagrams that illustrate the arrangement of clusters in hierarchical clustering. They show how clusters are merged or split at each step of the clustering process and provide insights into the relationships between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f09a4-7f8c-472d-b643-0f54453f2fbb",
   "metadata": {},
   "source": [
    "6.Applicability to numerical and categorical data:\n",
    "Hierarchical clustering can be used for both numerical and categorical data. Distance metrics differ for each type of data:\n",
    "\n",
    "For numerical data: Euclidean distance, Manhattan distance, etc.\n",
    "For categorical data: Jaccard distance, Hamming distance, etc.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bde9eb-e13d-4b31-a8a0-eff005b2ae41",
   "metadata": {},
   "source": [
    "7.Identifying outliers or anomalies:\n",
    "Hierarchical clustering can be used to identify outliers by observing clusters with a small number of members or clusters that are far from the rest. Data points that do not belong to any well-defined cluster or form singleton clusters can be considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cece543-32b2-4ee7-bc41-6c7414364345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
